Artificial Intelligence 1 Artificial Intelligence: Introduction to Neural Networks Perceptron, Backpropagation 2 Today â—¼ Neural Networks â‘ Perceptrons â‘ Backpropagation https://www.linkedin.com/pulse/goedels-incompleteness-theorem-emergence-ai-eberhard-schoeneburg/ 3 Neural Networks â—¼ Radically different approach to reasoning and learning â—¼ Inspired by biology â‘ the neurons in the human brain â—¼ Set of many simple processing units (neurons) connected together â—¼ Behavior of each neuron is very simple â‘ but a collection of neurons can have sophisticated behavior and can be used for complex tasks â—¼ In a neural network, the behavior depends on weights on the connection between the neurons â—¼ The weights will be learned given training data 4 Biological Neurons â—¼ Human brain = â‘ 100 billion neurons â‘ each neuron may be connected to 10,000 other neurons â‘ passing signals to each other via 1,000 trillion synapses â—¼ A neuron is made of: â‘ Dendrites: filaments that provide input to the neuron â‘ Axon: sends an output signal â‘ Synapses: connection with other neurons â€“ releases neurotransmitters to other neurons Source: http://www.human-memory.net/brain_neurons.html 5 Behavior of a Neuron â—¼ A neuron receives inputs from its neighbors â—¼ If enough inputs are received at the same time: â‘ the neuron is activated â‘ and fires an output to its neighbors â—¼ Repeated firings across a synapse increases its sensitivity and the future likelihood of its firing â—¼ If a particular stimulus repeatedly causes activity in a group of neurons, they become strongly associated 6 Today â—¼ Neural Networks â‘ Perceptrons â‘ Backpropagation https://www.linkedin.com/pulse/goedels-incompleteness-theorem-emergence-ai-eberhard-schoeneburg/ Feature Vector Representation â—¼ Sources of Feature Vector x â‘ Encoded image â‘ Tabulated data â‘ Embedded words â‘ â€¦ source: Luger (2005) 7 Feature Vector Representation â—¼ Sources of Feature Vector x â‘ Encoded image â‘ Tabulated data â‘ Embedded words â‘ â€¦ source: Luger (2005) E n c o d e r ğ‘µğŸ Ã— ğŸ ğ‘µ ğ‘µ 8 9 Feature Vector Representation â—¼ Sources of Feature Vector x â‘ Encoded image â‘ Tabulated data â‘ Embedded words â‘ â€¦ source: Luger (2005) E n c o d e r ğ‘µğŸ Ã— ğŸ 10 Feature Vector Representation â—¼ Sources of Feature Vector x â‘ Encoded image â‘ Tabulated data â‘ Embedded words â‘ â€¦ source: Luger (2005) E n c o d e r ğ‘µğŸ Ã— ğŸ 11 A Perceptron Network â—¼ Goal: Map Input Feature Vector x into Output Feature Vector y 12 A Perceptron Network â—¼ Goal: Map Input Feature Vector x into Output Feature Vector y 13 A Perceptron Network â—¼ Goal: Map Input Feature Vector x into Output Feature Vector y 14 A Perceptron Network â—¼ Goal: Map Input Feature Vector x into Output Feature Vector y 15 A Perceptron Network â—¼ Goal: Map Input Feature Vector x into Output Feature Vector y jâ€™th Cell 16 A Perceptron Network â—¼ Goal: Map Input Feature Vector x into Output Feature Vector y jâ€™th Cell? 17 A Perceptron Network 18 A Perceptron Network 19 A Perceptron Network 20 A Perceptron Network 21 A Perceptron Network 22 A Perceptron Network 23 A Perceptron Network 24 A Perceptron Network 25 A Perceptron Network 26 A Perceptron Network 27 Fully Connected (FC) Network E n c o d e r FC ğ‘1 ğ‘2 . . ğ‘ğ‘€ 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 Applications of Neural Networks â—¼ Handwritten digit recognition â‘ Training set = set of handwritten digits (0â€¦9) â‘ Task: given a bitmap, determine what digit it represents â‘ Input: 1 feature for each pixel of the bitmap â‘ Output: 1 output unit for each possible character (only 1 should be activated) â‘ After training, network should work for fonts (handwriting) never encountered â—¼ Related pattern recognition applications: â‘ recognize postal codes â‘ recognize signatures â‘ â€¦ 48 Applications of Neural Networks â—¼ Speech synthesis â‘ Learning to pronounce English words â‘ Difficult task for a rule-based system because English pronunciation is highly irregular â‘ Examples: â—¼ letter â€œcâ€ can be pronounced [k] (cat) or [s] (cents) â—¼ Woman vs Women â‘ NETtalk: â—¼ uses the context and the letters around a letter to learn how to pronounce a letter â—¼ Input: letter and its surrounding letters â—¼ Output: phoneme 49 NETtalk Architecture â—¼ Network is made of 3 layers of units â—¼ input unit corresponds to a 7 character window in the text â—¼ each position in the window is represented by 29 input units (26 letters + 3 for punctuation and spaces) â—¼ 26 output units â€“ one for each possible phoneme Ex: a cat â†’ c is pronounced K source: Luger (2005) Listen to the output through iterations: https://www.youtube.com/watch?v=gakJlr3GecE https://www.youtube.com/watch?v=gakJlr3GecE https://www.youtube.com/watch?v=gakJlr3GecE 50 Neural Networks â—¼ Disadvantage: â‘ result is not easy to understand by humans (set of weights compared to decision tree)â€¦ it is a black box â—¼ Advantage: â‘ robust to noise in the input (small changes in input do not normally cause a change in output) and graceful degradation 52 Today â—¼ Introduction to Neural Networks â‘ Perceptrons â‘ Backpropagation